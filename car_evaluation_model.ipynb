{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346df739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This task consists of analysing the car data using Spark ML library. This data set is composed\n",
    "# of 1727 rows and 6 different attributes which are buying price, price of maintenance, number\n",
    "# of doors, capacity in terms of persons to carry, the relative size of luggage boot and the\n",
    "# estimated safety value of each car.\n",
    "\n",
    "# Build and evaluate any 2 Machine Learning algorithm using\n",
    "# Apache Sparkâ€™s ML library on this dataset. The decision tree will classify the type of the car -\n",
    "# 1) unacceptable, 2) acceptable, 3) good or 4) very good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9ad570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/srijachatterjee/anaconda3/lib/python3.10/site-packages (3.4.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/srijachatterjee/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af4e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3038ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58aa5277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/31 08:52:47 WARN Utils: Your hostname, Srijas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.99 instead (on interface en0)\n",
      "23/07/31 08:52:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/31 08:52:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/31 08:53:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Car Classification\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c5d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the combined dataframe: 1727\n",
      "Number of columns in the combined dataframe: 7\n",
      "+--------+---------+-------+---------+-----------+------+--------+\n",
      "|buyPrice|maintCost|noDoors|noPersons|bootLuggage|safety|decision|\n",
      "+--------+---------+-------+---------+-----------+------+--------+\n",
      "|   vhigh|    vhigh|      2|        2|      small|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|      small|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        med|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        med|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        med|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        big|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        big|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        2|        big|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|      small|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|      small|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|      small|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        med|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        med|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        med|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        big|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        big|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|        4|        big|  high|   unacc|\n",
      "|   vhigh|    vhigh|      2|     more|      small|   low|   unacc|\n",
      "|   vhigh|    vhigh|      2|     more|      small|   med|   unacc|\n",
      "|   vhigh|    vhigh|      2|     more|      small|  high|   unacc|\n",
      "+--------+---------+-------+---------+-----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The csv files added to a files array\n",
    "files = [\"car_evaluation_0.csv\",\"car_evaluation_1.csv\", \"car_evaluation_2.csv\", \"car_evaluation_3.csv\"] \n",
    "\n",
    "data = []\n",
    "# for each .csv file in the files array, read the .csv files with header being True signifying \n",
    "# the first line is the header\n",
    "#  spark.read.csv will load the data into a DataFrame object using Apache Spark\n",
    "for file_path in files:\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    data.append(df) #append dataframe df into data array \n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "\n",
    "# Initialize combined_df with the first DataFrame which is the \"car_evaluation_0\" content, from data dataframe\n",
    "combined_df = data[0]\n",
    "\n",
    "# Loop through the remaining dataFrames starting from \"car_evaluation_1.csv\" to \"car_evaluation_3.csv\" \n",
    "# and concatenate them to combined_df\n",
    "# union concatenates with combined_df and select will make sure the other dataframes have the same columns as first dataframe i.e\n",
    "# \"car_evaluation_0\" content\n",
    "\n",
    "for df in data[1:]:\n",
    "    combined_df = combined_df.union(df.select(*combined_df.columns))\n",
    "\n",
    "# Count the number of rows and columns in the combined dataframe to confirm if all files were combined, should \n",
    "# return 1727 rows and 7 columns\n",
    "# 432 rows from \"car_evaluation_0.csv\", 432 rows from \"car_evaluation_1.csv\",\n",
    "# 432 rows from \"car_evaluation_2.csv\",431 rows from \"car_evaluation_3.csv\".\n",
    "\n",
    "rowCount = combined_df.count()\n",
    "colCount = len(combined_df.columns)\n",
    "\n",
    "\n",
    "print(\"Number of rows in the combined dataframe:\", rowCount)\n",
    "print(f\"Number of columns in the combined dataframe: {colCount}\")\n",
    "\n",
    "combined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d47376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------+---------------+-----------------+------------+--------------+\n",
      "|buyPrice_index|maintCost_index|noDoors_index|noPersons_index|bootLuggage_index|safety_index|decision_index|\n",
      "+--------------+---------------+-------------+---------------+-----------------+------------+--------------+\n",
      "|           3.0|            3.0|          3.0|            2.0|              2.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              2.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              1.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              1.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              1.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              0.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              0.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            2.0|              0.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              2.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              2.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              2.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              1.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              1.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              1.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              0.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              0.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            0.0|              0.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              2.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              2.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              2.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              1.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              1.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              1.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              0.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              0.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          3.0|            1.0|              0.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          0.0|            2.0|              2.0|         2.0|           0.0|\n",
      "|           3.0|            3.0|          0.0|            2.0|              2.0|         1.0|           0.0|\n",
      "|           3.0|            3.0|          0.0|            2.0|              2.0|         0.0|           0.0|\n",
      "|           3.0|            3.0|          0.0|            2.0|              1.0|         2.0|           0.0|\n",
      "+--------------+---------------+-------------+---------------+-----------------+------------+--------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "+---------+---------+-------------+-------------+-------------+-------------+\n",
      "|buying_en| maint_en|     doors_en|   persons_en|       lug_en|    safety_en|\n",
      "+---------+---------+-------------+-------------+-------------+-------------+\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|    (2,[],[])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|    (2,[],[])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[1],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[1],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[0],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|    (2,[],[])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|    (2,[],[])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|    (2,[],[])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|    (2,[],[])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[1],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[1],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[0],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|    (2,[],[])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|    (2,[],[])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|    (2,[],[])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[1],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[1],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[0],[1.0])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|    (3,[],[])|(2,[1],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|(3,[0],[1.0])|    (2,[],[])|    (2,[],[])|    (2,[],[])|\n",
      "|(3,[],[])|(3,[],[])|(3,[0],[1.0])|    (2,[],[])|    (2,[],[])|(2,[1],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|(3,[0],[1.0])|    (2,[],[])|    (2,[],[])|(2,[0],[1.0])|\n",
      "|(3,[],[])|(3,[],[])|(3,[0],[1.0])|    (2,[],[])|(2,[1],[1.0])|    (2,[],[])|\n",
      "+---------+---------+-------------+-------------+-------------+-------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "+--------------------+--------------+\n",
      "|            features|decision_index|\n",
      "+--------------------+--------------+\n",
      "|     (15,[14],[1.0])|           0.0|\n",
      "|     (15,[13],[1.0])|           0.0|\n",
      "|     (15,[12],[1.0])|           0.0|\n",
      "|(15,[12,14],[1.0,...|           0.0|\n",
      "|(15,[12,13],[1.0,...|           0.0|\n",
      "|     (15,[11],[1.0])|           0.0|\n",
      "|(15,[11,14],[1.0,...|           0.0|\n",
      "|(15,[11,13],[1.0,...|           0.0|\n",
      "|      (15,[9],[1.0])|           0.0|\n",
      "|(15,[9,14],[1.0,1...|           0.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# The data needs to be converted into numbers to feed into the machine learning model and as single features \n",
    "# Then we use these features in our ML models\n",
    "\n",
    "\n",
    "selected_data =[]\n",
    "\n",
    "# Perform StringIndexing on categorical columns\n",
    "# replace each category with a number since machine learning models need to be fed with numerical data \n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(combined_df) for col in [\"buyPrice\", \"maintCost\", \"noDoors\", \"noPersons\", \"bootLuggage\",\"safety\",\"decision\"]]\n",
    "\n",
    "# A Pipeline is used to chain multiple Transformers and Estimators together to specify our machine learning workflow\n",
    "pipeline = Pipeline(stages=indexers) #pipeline to chain the transformers and estimators \n",
    "\n",
    "\n",
    "\n",
    "# Next we call the fit() method to initiate the learning process and transform to add the indexed columns\n",
    "indexed_data = pipeline.fit(combined_df).transform(combined_df)\n",
    "\n",
    "# The returned indexer_model is an object of type Transformer.A Transformer is an abstraction that includes feature transformers and learned models. It\n",
    "# implements a method transform(), which converts one DataFrame into another, generally by\n",
    "# appending one or more columns.\n",
    "\n",
    "indexed_data.select(\"buyPrice_index\", \"maintCost_index\", \"noDoors_index\", \"noPersons_index\", \"bootLuggage_index\",\"safety_index\",\"decision_index\").show(n=30)\n",
    "\n",
    "# Perform OneHotEncoding on indexed columns\n",
    "encoder = OneHotEncoder(inputCols=[\"buyPrice_index\", \"maintCost_index\", \"noDoors_index\", \"noPersons_index\", \"bootLuggage_index\",\"safety_index\"],\n",
    "                        outputCols=[\"buying_en\", \"maint_en\", \"doors_en\", \"persons_en\", \"lug_en\", \"safety_en\"])\n",
    "\n",
    "# After fitting the estimator and getting our transformer, it is time to use it on our data by\n",
    "# calling transform().\n",
    "encoded_data = encoder.fit(indexed_data).transform(indexed_data)\n",
    "\n",
    "# show the first 30 rows after one hot encoding the dataframe\n",
    "encoded_data.select(\"buying_en\", \"maint_en\", \"doors_en\", \"persons_en\", \"lug_en\", \"safety_en\").show(n=30)\n",
    "\n",
    "\n",
    "# We learned previously that Spark ML expects data to be represented in two columns:\n",
    "# a features vector and a label column.\n",
    "\n",
    "# To create a features array use VectorAssembler and add the encoded columns to add as combined features column\n",
    "assembler = VectorAssembler(inputCols=[\"buying_en\", \"maint_en\", \"doors_en\", \"persons_en\", \"lug_en\", \"safety_en\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# assembler used to create the features column\n",
    "selected_data = assembler.transform(encoded_data)\n",
    "\n",
    "# show the features and decision columns since they'll be used as input for the machine learning models next\n",
    "selected_data.select(\"features\", \"decision_index\").show(n=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd5b2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spark modules for implimenting Decision tree and random forests classifiers \n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fde4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1399\n",
      "Test Dataset Count: 328\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "(train, test) = selected_data.randomSplit([0.8, 0.2], seed=123)\n",
    "# (train, test) = selected_data.randomSplit([0.8, 0.2], seed=42)\n",
    "# (train, test) = selected_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ec04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "|buyPrice|maintCost|noDoors|noPersons|bootLuggage|safety|decision|decision_index|prediction|      probability|\n",
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "|   vhigh|     high|      2|        2|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        2|        med|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|        big|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|      small|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|     more|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        big|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        med|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|      small|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|      small|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        4|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        4|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|     more|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|        2|        big|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|        2|      small|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|     more|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|     more|      small|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|  5more|        2|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|  5more|        4|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.97       241\n",
      "         1.0       0.77      0.87      0.82        55\n",
      "         2.0       0.85      0.73      0.79        15\n",
      "         3.0       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.92       328\n",
      "   macro avg       0.85      0.82      0.83       328\n",
      "weighted avg       0.93      0.92      0.92       328\n",
      "\n",
      "[[232   9   0   0]\n",
      " [  6  48   0   1]\n",
      " [  0   2  11   2]\n",
      " [  0   3   2  12]]\n",
      "Decision tree accuracy: 92.38%\n",
      "Decision tree accuracy test Error: 7.62%\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Decision Tree\n",
    "# dt = DecisionTreeClassifier(labelCol=\"decision_index\", featuresCol=\"features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    labelCol=\"decision_index\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=2,\n",
    "    impurity=\"gini\",\n",
    ")\n",
    "\n",
    "\n",
    "# dt = DecisionTreeClassifier(labelCol=\"decision_index\", featuresCol=\"features\",maxDepth = 3)\n",
    "decision_tree_evaluator = MulticlassClassificationEvaluator(labelCol=\"decision_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Set up a parameter grid for tuning the decision tree model\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "         \n",
    "# Build a CrossValidator to find the best model using the parameter grid\n",
    "cv_dt = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=decision_tree_evaluator, numFolds=5)\n",
    "\n",
    "# Train the models\n",
    "dt_model = cv_dt.fit(train)\n",
    "\n",
    "# Predicting Values\n",
    "predictions = dt_model.transform(test)\n",
    "\n",
    "# predictions is a DataFrame that contains: the original columns, the features column and \n",
    "# predictions column generated by the model\n",
    "predictions.select(['buyPrice', 'maintCost', 'noDoors', 'noPersons', 'bootLuggage','safety','decision','decision_index','prediction', 'probability']).show()\n",
    "\n",
    "y_true = predictions.select(['decision_index']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "dt_accuracy = decision_tree_evaluator.evaluate(predictions)\n",
    "# Evaluate the models on the testing data\n",
    "print(\"Decision tree accuracy: {:.2f}%\".format(dt_accuracy * 100))\n",
    "print(\"Decision tree accuracy test Error: {:.2f}%\".format((1.0 - dt_accuracy) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32a069b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "|buyPrice|maintCost|noDoors|noPersons|bootLuggage|safety|decision|decision_index|prediction|      probability|\n",
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "|   vhigh|     high|      2|        2|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        2|        med|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|        big|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|        4|      small|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      2|     more|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        big|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|        med|   med|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|      small|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        2|      small|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        4|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|        4|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      3|     more|        big|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|        2|        big|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|        2|      small|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|     more|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|      4|     more|      small|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|  5more|        2|        med|  high|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "|   vhigh|     high|  5more|        4|        med|   low|   unacc|           0.0|       0.0|[1.0,0.0,0.0,0.0]|\n",
      "+--------+---------+-------+---------+-----------+------+--------+--------------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+----------+\n",
      "|decision_index|prediction|\n",
      "+--------------+----------+\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97       241\n",
      "         1.0       0.72      0.76      0.74        55\n",
      "         2.0       0.46      0.40      0.43        15\n",
      "         3.0       0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.89       328\n",
      "   macro avg       0.72      0.68      0.70       328\n",
      "weighted avg       0.89      0.89      0.89       328\n",
      "\n",
      "[[235   6   0   0]\n",
      " [  8  42   4   1]\n",
      " [  0   6   6   3]\n",
      " [  0   4   3  10]]\n",
      "Random forests accuracy: 89.33%\n",
      "Random forests test Error: 10.67%\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"decision_index\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=1,\n",
    "    maxDepth=30,\n",
    "    minInstancesPerNode=3,\n",
    "    featureSubsetStrategy=\"auto\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"decision_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Set up a parameter grid for tuning the Random Forest model \n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# Build a CrossValidator to find the best model using the parameter grid\n",
    "cv_rf = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=rf_evaluator, numFolds=5)\n",
    "\n",
    "# Train the models\n",
    "rf_model = cv_rf.fit(train)\n",
    "\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_model.transform(test))\n",
    "\n",
    "# Predicting Values\n",
    "predictions = rf_model.transform(test)\n",
    "# predictions is a DataFrame that contains: the original columns, the features column and \n",
    "# predictions column generated by the model\n",
    "\n",
    "predictions.select(['buyPrice', 'maintCost', 'noDoors', 'noPersons', 'bootLuggage','safety','decision','decision_index','prediction', 'probability']).show()\n",
    "# compare the preidcted value and the actual label for what was supposed to be predicted i.e the decision that was indexed\n",
    "predictions.select(['decision_index','prediction']).show()\n",
    "\n",
    "\n",
    "y_true = predictions.select(['decision_index']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Evaluate the models on the testing data\n",
    "print(\"Random forests accuracy: {:.2f}%\".format(rf_accuracy * 100))\n",
    "print(\"Random forests test Error: {:.2f}%\".format((1.0 - rf_accuracy) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe939635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|decision_index|decision|\n",
      "+--------------+--------+\n",
      "|             0|   unacc|\n",
      "|             1|     acc|\n",
      "|             2|    good|\n",
      "|             3|   vgood|\n",
      "+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the original labels from the StringIndexerModel from the beginning\n",
    "# Create a DataFrame to map original labels with their indexed labels\n",
    "prediction_labels = [(original_label, index_label) for original_label, index_label in enumerate(original_labels)]\n",
    "mappings = spark.createDataFrame(prediction_labels, [\"decision_index\", \"decision\"])\n",
    "\n",
    "# show the indexed labels mapped to the decision of car type  \n",
    "mappings.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd061dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
